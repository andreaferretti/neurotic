* separate gradint computation and update
* batch forward and backward propagation
* add training and testing functions
* gradient descent
* check that we actually learn on MNIST
* model.add
* clarify layer vs. model
* more cost functions
* more activation functions
* more optimization methods http://sebastianruder.com/optimizing-gradient-descent/
* 32-bit layers
* GPU layers
* static dimension layers
* weight sharing and RNN
* replicate standard examples
* save/load models
* load from model zoo
* layer visualization
* convolutional networks
* regularization
* momentum
* allow users to custom initialize layers
* choose whether to use iface or other interface implementations