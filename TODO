* separate gradient computation and update
* add training and testing functions
* add a function to gather statistics
* more cost functions
* more activation functions
* more optimization methods http://sebastianruder.com/optimizing-gradient-descent/
* more connectors (side by side etc)
* GPU layers
* nicer API
* static dimension layers
* weight sharing and RNN
* replicate standard examples
* save/load models
* load from model zoo
* layer visualization
* convolutional networks
* regularization
* momentum
* dropout
* add a function to export the neuron graph, say in dot
* allow users to custom initialize layers